# Day 14. 100 Days of Data Science Challenge - 02/14/2025

# ğŸ‘• Fashion Vision: CNN vs. ResNet Showdown  

Deep learning and fashionâ€”whatâ€™s not to love? For todayâ€™s challenge, I built an **image classification model** that can recognize different types of clothing. The dataset includes:  
ğŸ‘Ÿ **Shoes** | ğŸ‘š **T-Shirts** | ğŸ‘– **Bottoms**  

To solve this classification problem, I tested **two different approaches**:  
1ï¸âƒ£ **Fine-tuning a Pretrained ResNet50 Model**  
2ï¸âƒ£ **Building a Convolutional Neural Network (CNN) from Scratch**  

Which one is better? Letâ€™s find out! ğŸš€  

---

## ğŸ—‚ The Dataset  
This dataset consists of **300 images (100 per class)** collected and organized into folders.  
To ensure proper training, the dataset was split into:  
- **Training Set (64%)** â€“ The model learns from this.  
- **Validation Set (16%)** â€“ Used to tune hyperparameters.  
- **Test Set (20%)** â€“ Final evaluation to measure performance.  

### ğŸ”¹ Preprocessing & Data Augmentation  
Before feeding images into the models, I applied **preprocessing and augmentation** to improve generalization:  
âœ”ï¸ **Resizing** all images to **224 Ã— 224 pixels**.  
âœ”ï¸ **Normalizing** pixel values to the `[0,1]` range.  
âœ”ï¸ **Data Augmentation** (only for training data):  
   - **Random Rotation**  
   - **Shifting & Shearing**  
   - **Zooming & Flipping**  

These techniques helped **prevent overfitting** and ensured that the model learned robust patterns!  

---

## ğŸ›  The Models  

### ğŸ”¹ **1. Transfer Learning: ResNet50**  
**Why ResNet50?** Itâ€™s a powerful, pretrained convolutional neural network that has already learned high-level image features from millions of images.  
Hereâ€™s how I adapted it for this task:  
- Used a **pretrained ResNet50 model** as a feature extractor.  
- **Removed the top layers** and added a custom classification head.  
- **Froze the base layers** to retain pretrained knowledge.  
- **Used Adam optimizer** with **categorical crossentropy loss**.  

ğŸ’¡ **Results:** The model trained quickly but had a **test accuracy of ~61.67%**. Not bad, but I wanted to see if I could do better!  

---

### ğŸ”¹ **2. Custom CNN (From Scratch)**  
I designed and trained a **Convolutional Neural Network (CNN) from scratch**.  
Hereâ€™s the architecture:  
1ï¸âƒ£ **Conv2D Layer (32 filters, 3Ã—3 kernel)** â€“ Extracts low-level patterns.  
2ï¸âƒ£ **MaxPooling Layer** â€“ Reduces spatial dimensions.  
3ï¸âƒ£ **Conv2D Layer (64 filters, 3Ã—3 kernel)** â€“ Captures more complex features.  
4ï¸âƒ£ **MaxPooling Layer** â€“ Further reduces size.  
5ï¸âƒ£ **Flatten Layer** â€“ Converts feature maps into a 1D vector.  
6ï¸âƒ£ **Dense Layer (512 neurons, ReLU activation)** â€“ Fully connected for learning.  
7ï¸âƒ£ **Dense Layer (3 neurons, Softmax activation)** â€“ Outputs class probabilities.  

ğŸ’¡ **Results:** **This model performed better than ResNet50!**  
Although training took longer, it generalized well and achieved higher accuracy.  

---

## ğŸ“Š Results & Key Takeaways  
Hereâ€™s what I learned from comparing both models:  
ğŸ“Œ **CNN (Trained from Scratch) > ResNet50 in test accuracy.**  
ğŸ“Œ **Transfer learning is fast but requires fine-tuning.**  
ğŸ“Œ **More data = Better CNN performance.**  
ğŸ“Œ **Data augmentation helped the CNN generalize better.**  
ğŸ“Œ **Pretrained models may not always be the best solution!**  

I also visualized **correct vs. incorrect predictions** to analyze where the models struggled.  

---

## ğŸ“‰ Training Performance  
### ğŸ”¹ **Loss & Accuracy Plots**  
I plotted **training vs. validation loss and accuracy** to understand model performance.  

- **Loss Observation:**  
  - The CNN model showed a **steady decrease in training loss**, while the validation loss fluctuated slightly before stabilizing.  
  - ResNet50 initially performed well but plateaued early.  

- **Accuracy Observation:**  
  - The CNN model improved steadily and **outperformed ResNet50 on the test set**.  
  - ResNet50 started strong but struggled to generalize.  

---

## ğŸš€ Whatâ€™s Next?  
ğŸ”¹ **Try deeper CNN architectures** to push accuracy even further.  
ğŸ”¹ **Experiment with other pretrained models** (EfficientNet, MobileNet).  
ğŸ”¹ **Fine-tune ResNet50 more aggressively** (adjust learning rates, unfreeze more layers).  
ğŸ”¹ **Collect more data** to enhance model robustness.  

---

## ğŸ’¡ Reflections
Day 14 of 100 Days of Data Science was an eye-opener!

ğŸ”¹Custom models can sometimes outperform pretrained ones!

ğŸ”¹Preprocessing & augmentation are critical for good performance.

ğŸ”¹Experimenting with different architectures is key to deep learning success.

This was an exciting project, and I canâ€™t wait to explore more deep learning challenges! ğŸš€

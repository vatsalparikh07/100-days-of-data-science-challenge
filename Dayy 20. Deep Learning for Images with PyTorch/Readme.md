# Day 20. 100 Days of Data Science Challenge - 02/20/2025

# ğŸ–¼ï¸ Binary Image Classification with PyTorch

## ğŸ” Project Overview  

In this project, I explored **Deep Learning** using **ResNet18** in **PyTorch** to classify images into two categories:  

âœ… **Sloths** ğŸ¦¥  
âœ… **Pain au Chocolat** ğŸ¥  

Instead of training a deep neural network from scratch, I **fine-tuned a pre-trained model** (ResNet18) to recognize our specific classes. This significantly reduces the training time while achieving **high accuracy** by leveraging pre-learned features from ImageNet.  

**ğŸ¯ Key Objectives:**  
âœ” Understand **feature extraction vs. fine-tuning** in Transfer Learning  
âœ” Implement **image preprocessing & augmentation** for robust classification  
âœ” Train a **deep learning model using PyTorch**  
âœ” Optimize performance using **learning rate scheduling & momentum-based optimizers**  
âœ” Evaluate the model using **accuracy metrics & visualization techniques**  

---

## ğŸ—‚ Dataset & Preprocessing  

The dataset consists of **two categories (sloths & pain au chocolat)** and is structured into **training & validation sets** as follows:  

Before training, images undergo **multiple preprocessing steps**:  

ğŸ”¹ **Resizing & Normalization** â€“ Images are resized to **224Ã—224 pixels** and normalized to match ResNet18â€™s input requirements.  
ğŸ”¹ **Data Augmentation** â€“ Applied **random cropping, flipping, and normalization** to prevent overfitting and improve generalization.  

---
## ğŸ— Model Architecture & Training  

This project utilizes **ResNet18**, a deep convolutional neural network pre-trained on **ImageNet**. The **final fully connected (FC) layer** is modified to classify images into **two categories** (sloths or pain au chocolat).  

### ğŸ”¹ Transfer Learning Approach  

âœ” **Feature Extraction** â€“ Freeze early layers and train only the final layer.  
âœ” **Fine-Tuning** â€“ Unfreeze select layers and optimize for domain-specific classification.  

### ğŸ”¹ Training Configuration  

| Parameter            | Value                     |
|----------------------|--------------------------|
| **Model**           | ResNet18 (pre-trained)    |
| **Loss Function**   | CrossEntropyLoss          |
| **Optimizer**       | SGD with momentum         |
| **Learning Rate**   | 0.001 (decayed every 7 epochs) |
| **Batch Size**      | 4                          |
| **Epochs**         | 25                         |

To optimize the learning process, **StepLR learning rate scheduling** is applied to adjust the learning rate dynamically.  

---

## ğŸ“Š Model Performance  

**ğŸ† Final Validation Accuracy: 100%**  

| Epoch | Train Accuracy | Validation Accuracy |
|-------|---------------|---------------------|
| 1     | 87.43%        | 97.73%              |
| 5     | 91.02%        | 100.00%             |
| 10    | 95.81%        | 100.00%             |
| 15    | 94.61%        | 100.00%             |
| 20    | 95.21%        | 100.00%             |
| 25    | 96.41%        | 100.00%             |

ğŸ’¡ **Observations:**  
âœ” **Transfer Learning significantly reduced training time** while maintaining high accuracy.  
âœ” **Augmentation improved model robustness**, preventing overfitting.  
âœ” **Using a learning rate scheduler** helped in better model convergence.  

---

## ğŸ¨ Model Predictions & Visualizations  

After training, the model is evaluated by visualizing **classified images** from the validation set. The predictions are displayed alongside ground truth labels to assess model performance.  

ğŸ”¹ **Correct Predictions:** Model successfully identifies sloths & pastries.  
ğŸ”¹ **Edge Cases:** Some misclassifications occur in images with **similar textures or backgrounds**.  

---

## ğŸ† Key Takeaways  

âœ” **Transfer Learning enables high accuracy with minimal training**  
âœ” **Fine-tuning the final layer** was enough for effective classification  
âœ” **Data augmentation** significantly improves model robustness  
âœ” **Using SGD with momentum & learning rate scheduling** optimizes training  
âœ” **Model generalizes well**, achieving **100% validation accuracy**  


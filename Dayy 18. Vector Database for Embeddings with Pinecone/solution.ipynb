{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Databases for Embeddings with Pinecone"
      ],
      "metadata": {
        "id": "yo1pGPkn97nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Pinecone Client\n",
        "\n",
        "### Question:\n",
        "Throughout the course, you'll write Python code to interact with Pinecone via the Pinecone Python client. As a first step, you'll need to create your own Pinecone API key. Pinecone API keys used in this course's exercises will not be stored in any way.\n",
        "\n",
        "To create a key, you'll first need to create a Pinecone starter account, which is free, by visiting [Pinecone's website](https://www.pinecone.io/). Next, navigate to the API Keys page to create your key.\n",
        "\n",
        "### Instructions:\n",
        "- Import the class used to create a Pinecone client from `pinecone`.\n",
        "- Instantiate the Pinecone class, passing in your API key."
      ],
      "metadata": {
        "id": "SieJET9F9SM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nrpDh7an8vCm"
      },
      "outputs": [],
      "source": [
        "# Import the Pinecone library\n",
        "from pinecone import Pinecone\n",
        "pc = Pinecone(api_key = \"api_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Your First Pinecone Index\n",
        "\n",
        "### Question:\n",
        "With your Pinecone client initialized, you're all set to begin creating an index! Indexes are used to store records, including the vectors and associated metadata, as well as serving queries and other manipulations. As you progress through the course, you'll see how these different steps build up to a modern AI system built on a vector database.\n",
        "\n",
        "If you accidentally create a valid index that doesn't meet the specifications detailed in the instructions, you'll need to add the following code before your `.create_index()` code to delete it and re-create it:\n",
        "\n",
        "```python\n",
        "pc.delete_index('my-first-index')\n"
      ],
      "metadata": {
        "id": "K7HKcKwM9-n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "# Create your Pinecone index\n",
        "pc.create_index(\n",
        "    name='my-first-index',\n",
        "    dimension=256,\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFD1HA3v9_TS",
        "outputId": "a397eb01-849f-4209-f1c2-7a700d13294b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "    \"name\": \"my-first-index\",\n",
              "    \"metric\": \"cosine\",\n",
              "    \"host\": \"my-first-index-b2pnslq.svc.aped-4627-b74a.pinecone.io\",\n",
              "    \"spec\": {\n",
              "        \"serverless\": {\n",
              "            \"cloud\": \"aws\",\n",
              "            \"region\": \"us-east-1\"\n",
              "        }\n",
              "    },\n",
              "    \"status\": {\n",
              "        \"ready\": true,\n",
              "        \"state\": \"Ready\"\n",
              "    },\n",
              "    \"vector_type\": \"dense\",\n",
              "    \"dimension\": 256,\n",
              "    \"deletion_protection\": \"disabled\",\n",
              "    \"tags\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Connecting to an Index\n",
        "\n",
        "### Question:\n",
        "To begin ingesting vectors and performing vector manipulations in your newly-created Pinecone index, you'll first need to connect to it! The resulting index object has a number of methods for ingesting, manipulating, and exploring the contents of the index in Python.\n",
        "\n",
        "The `Pinecone` class has already been imported for you and will be available throughout the course.\n",
        "\n",
        "### Instructions:\n",
        "- Initialize the Pinecone connection with your API key.\n",
        "- Connect to the `\"my-first-index\"` index.\n",
        "- Print key statistics about the index using an index method."
      ],
      "metadata": {
        "id": "jtJetzMQ-RJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to your index\n",
        "index = pc.Index(\"my-first-index\")\n",
        "\n",
        "# Print the index statistics\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufQAWt4R-I9a",
        "outputId": "4062617b-9bc5-419c-bc83-b7f39d08d2e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 256,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Deleting an Index\n",
        "\n",
        "### Question:\n",
        "If you have an index that has gone stale, perhaps it's time to delete it! **Deleting an index will also delete all of the data it contains, so be cautious when doing this in your own projects!**\n",
        "\n",
        "### Instructions:\n",
        "- Initialize the Pinecone connection with your API key.\n",
        "- Delete the index you've been working with: `\"my-first-index\"`.\n",
        "- List your indexes to verify it has been deleted."
      ],
      "metadata": {
        "id": "-3LAvOtM-gCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete your Pinecone index\n",
        "pc.delete_index('my-first-index')\n",
        "\n",
        "# List your indexes\n",
        "print(pc.list_indexes())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX4D14jz-goz",
        "outputId": "56720069-6fb3-4d08-a360-b4ec276e04c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\n",
            "    \"name\": \"datacamp-index\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"datacamp-index-b2pnslq.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"pinecone-datacamp\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"pinecone-datacamp-b2pnslq.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"dotproduct-index\",\n",
            "    \"metric\": \"dotproduct\",\n",
            "    \"host\": \"dotproduct-index-b2pnslq.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Checking Dimensionality\n",
        "\n",
        "### Question:\n",
        "You now have the know-how to begin ingesting vectors into a new Pinecone index! Before you jump in, you should check that your vectors are compatible with the dimensionality of your new index.\n",
        "\n",
        "A list of dictionaries containing records to ingest has been provided as `vectors`. Here's a preview of its structure:\n",
        "\n",
        "```python\n",
        "vectors = [\n",
        "    {\n",
        "        \"id\": \"0\",\n",
        "        \"values\": [0.025525547564029694, ..., 0.0188823901116848],\n",
        "        \"metadata\": {\"genre\": \"action\", \"year\": 2024}\n",
        "    },\n",
        "    ...\n",
        "]\n"
      ],
      "metadata": {
        "id": "rSd6II27-juz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = [\n",
        "    {\n",
        "        \"id\": \"0\",\n",
        "        \"values\": [0.0] * 1536,  # Example 1536-dimensional vector\n",
        "        \"metadata\": {\"genre\": \"action\", \"year\": 2024}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"values\": [0.1] * 1536,\n",
        "        \"metadata\": {\"genre\": \"comedy\", \"year\": 2023}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Check that each vector has a dimensionality of 1536\n",
        "vector_dims = [len(vector[\"values\"]) == 1536 for vector in vectors]\n",
        "print(\"All vectors have the correct dimensionality:\", all(vector_dims))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnqpQgG-7rL",
        "outputId": "a82c35da-199c-476e-9831-f0ef19c1f84b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All vectors have the correct dimensionality: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Ingesting Vectors with Metadata\n",
        "\n",
        "### Question:\n",
        "It's ingesting time! You'll be ingesting vectors, which is a list of dictionaries containing the vector values, IDs, and associated metadata. They have already been provided in a format that can be directly inserted into the index without further manipulation.\n",
        "\n",
        "Here's another reminder about the structure of `vectors`:\n",
        "\n",
        "```python\n",
        "vectors = [\n",
        "    {\n",
        "        \"id\": \"0\",\n",
        "        \"values\": [0.025525547564029694, ..., 0.0188823901116848],\n",
        "        \"metadata\": {\"genre\": \"action\", \"year\": 2024}\n",
        "    },\n",
        "    ...\n",
        "]\n"
      ],
      "metadata": {
        "id": "jxpfL7-__OO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the index name\n",
        "index_name = \"datacamp-index\"\n",
        "\n",
        "# Connect to your index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "vectors = [\n",
        "    {\n",
        "        \"id\": \"0\",\n",
        "        \"values\": [0.025525547564029694] * 1536,\n",
        "        \"metadata\": {\"genre\": \"action\", \"year\": 2024}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"values\": [0.0188823901116848] * 1536,\n",
        "        \"metadata\": {\"genre\": \"comedy\", \"year\": 2023}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format vectors for Pinecone upsert\n",
        "upsert_data = [(v[\"id\"], v[\"values\"], v[\"metadata\"]) for v in vectors]\n",
        "\n",
        "# Ingest the vectors and metadata\n",
        "index.upsert(vectors=upsert_data)\n",
        "\n",
        "# Print the index statistics\n",
        "stats = index.describe_index_stats()\n",
        "print(\"Index Statistics:\", stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB8jquSS_N7e",
        "outputId": "b2fd5cf5-9224-428c-e727-01a14698c936"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index Statistics: {'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'': {'vector_count': 1000},\n",
            "                'namespace1': {'vector_count': 50},\n",
            "                'namespace2': {'vector_count': 50}},\n",
            " 'total_vector_count': 1100,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Fetching Vectors\n",
        "\n",
        "### Question:\n",
        "In this exercise, you've been provided with a list of `ids` containing IDs of different records in your `'datacamp-index'` index. You'll use these IDs to retrieve the associated records and explore their metadata.\n",
        "\n",
        "### Instructions:\n",
        "- Initialize the Pinecone connection with your API key.\n",
        "- Retrieve the vectors with IDs in the `ids` list from the connected index.\n",
        "- Create a list of dictionaries containing the metadata from each record in `fetched_vectors`."
      ],
      "metadata": {
        "id": "r4CmguJh_RH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of IDs to fetch\n",
        "ids = ['0', '1']\n",
        "\n",
        "# Fetch vectors from the connected Pinecone index\n",
        "fetched_response = index.fetch(ids=ids)\n",
        "\n",
        "# Extract metadata safely\n",
        "metadatas = [\n",
        "    fetched_response.vectors[id].metadata\n",
        "    for id in ids if id in fetched_response.vectors\n",
        "]\n",
        "\n",
        "# Print metadata\n",
        "print(\"Fetched Metadata:\", metadatas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn9PWMG5_00e",
        "outputId": "ee7f2597-2e90-4c6a-ba70-7fe9f8f2b95c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched Metadata: [{'genre': 'action', 'year': 2024.0}, {'genre': 'comedy', 'year': 2023.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Returning the Most Similar Vectors  \n",
        "\n",
        "Querying vectors is foundational to many AI applications. It involves embedding a user input, comparing it to the vectors in the database, and returning the most similar vectors.  \n",
        "\n",
        "In this exercise, you've been provided with a mystery vector called `vector`, and you'll use it to query your index called **\"datacamp-index\"**.  \n",
        "\n",
        "### Instructions\n",
        "- Initialize the Pinecone connection with your API key.  \n",
        "- Retrieve the **three records** with vectors that are most similar to `vector`.  \n"
      ],
      "metadata": {
        "id": "Mz4Y4FPsAWKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example vector (ensure it matches the dimensionality of your index, e.g., 1536)\n",
        "vector = [0.01] * 1536  # Replace with an actual query vector\n",
        "\n",
        "# Retrieve the top three most similar records\n",
        "query_result = index.query(\n",
        "    vector=vector,  # The query vector\n",
        "    top_k=3,  # Number of most similar results\n",
        "    include_metadata=True  # Retrieve metadata for better context\n",
        ")\n",
        "\n",
        "# Print query results\n",
        "print(\"Most Similar Vectors:\", query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_6w13ZcAVV2",
        "outputId": "8187b2b2-5f10-4c86-a99e-6d531ac2c719"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Similar Vectors: {'matches': [{'id': '1',\n",
            "              'metadata': {'genre': 'comedy', 'year': 2023.0},\n",
            "              'score': 0.999999821,\n",
            "              'values': []},\n",
            "             {'id': '0',\n",
            "              'metadata': {'genre': 'action', 'year': 2024.0},\n",
            "              'score': 0.999999642,\n",
            "              'values': []},\n",
            "             {'id': '419', 'score': 0.0761137679, 'values': []}],\n",
            " 'namespace': '',\n",
            " 'usage': {'read_units': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering Queries  \n",
        "\n",
        "In this exercise, you'll practice querying the **\"datacamp-index\"** Pinecone index. You'll connect to the index and query it using the vector provided to retrieve similar vectors. You'll also use **metadata filtering** to optimize your querying and return the most relevant search results.  \n",
        "\n",
        "### Instructions\n",
        "- Initialize the Pinecone connection with your API key.  \n",
        "- Retrieve the **MOST similar** vector to the vector provided.  \n",
        "- Use **metadata filtering** to only search through vectors where the metadata **'year' equals 2024**.  \n"
      ],
      "metadata": {
        "id": "ZmYp-DycAtHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the MOST similar vector with the year 2024\n",
        "query_result = index.query(\n",
        "    vector = vector,\n",
        "    filter =  {\n",
        "        \"year\":{\"$eq\":2024}\n",
        "    },\n",
        "    top_k = 1\n",
        ")\n",
        "print(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F5J1F7bAswV",
        "outputId": "808ecb0b-7053-4556-c673-c71e60f81026"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [{'id': '0', 'score': 0.999999642, 'values': []}],\n",
            " 'namespace': '',\n",
            " 'usage': {'read_units': 5}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Metadata Filters  \n",
        "\n",
        "Pinecone allows for multiple **metadata filters** in a single query, enabling more precise search results. In this exercise, you'll query the **\"datacamp-index\"** Pinecone index using **multiple filters** with comparison operators.  \n",
        "\n",
        "### Instructions\n",
        "- Initialize the Pinecone connection with your API key.  \n",
        "- Retrieve the **MOST similar** vector to the vector provided.  \n",
        "- Use **metadata filtering** to **only** search through vectors where:  \n",
        "  - The **\"genre\"** metadata is `\"thriller\"`.  \n",
        "  - The **\"year\"** is **less than 2018**.  \n"
      ],
      "metadata": {
        "id": "eBcbPlDwA9L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the MOST similar vector with genre and year filters\n",
        "query_result = index.query(\n",
        "    vector=vector,\n",
        "    top_k=1,\n",
        "    filter={\n",
        "        \"genre\": {\"$eq\": \"thriller\"},\n",
        "        \"year\" : {\"$lt\": 2018}\n",
        "    }\n",
        ")\n",
        "print(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NiqRfuJA9bx",
        "outputId": "f7e8dbf0-e612-4402-d6a8-6c31df61a6fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [], 'namespace': '', 'usage': {'read_units': 5}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a function for chunking\n",
        "To be able to batch upserts in a reproducible way, you'll need to define a function to split your list of vectors into chunks.\n",
        "\n",
        "The built-in `itertools` module has already been imported for you.\n",
        "\n",
        "## Instructions\n",
        "- Convert the iterable input into an iterator.\n",
        "- Slice it into chunks of size `batch_size` using the `itertools` module.\n",
        "- Yield the current chunk.\n"
      ],
      "metadata": {
        "id": "6rgIGA0oCYOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def chunks(iterable, batch_size=100):\n",
        "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
        "    # Convert the iterable into an iterator\n",
        "    it = iter(iterable)\n",
        "    # Slice the iterator into chunks of size batch_size\n",
        "    chunk = tuple(itertools.islice(it, batch_size))\n",
        "    while chunk:\n",
        "        # Yield the chunk\n",
        "        yield chunk\n",
        "        chunk = tuple(itertools.islice(it, batch_size))"
      ],
      "metadata": {
        "id": "BfZY6BQRCYey"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching upserts in chunks\n",
        "In this exercise, you'll practice ingesting vectors into the `'datacamp-index'` Pinecone index in series, batch-by-batch.\n",
        "\n",
        "- Upsert the vectors in vectors in batches of 100 vectors into 'datacamp-index'.\n"
      ],
      "metadata": {
        "id": "jy6i1Ak0Cjjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert vectors in batches of 100\n",
        "for chunk in chunks(vectors, batch_size=100):\n",
        "    index.upsert(vectors=chunk)\n",
        "\n",
        "# Retrieve statistics of the connected Pinecone index\n",
        "print(index.describe_index_stats())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7njjMBjCzgR",
        "outputId": "b4b88759-ad79-4c27-baf2-34db5147dc9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'': {'vector_count': 1000},\n",
            "                'namespace1': {'vector_count': 50},\n",
            "                'namespace2': {'vector_count': 50}},\n",
            " 'total_vector_count': 1100,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching upserts in parallel\n",
        "In this exercise, you'll practice ingesting vectors into the `'datacamp-index'` Pinecone index in parallel. You'll need to connect to the index, upsert vectors in batches asynchronously, and check the updated metrics of the `'datacamp-index'` index.\n",
        "\n",
        "## Instructions\n",
        "1. Initialize the Pinecone client to allow **20 simultaneous requests**.\n",
        "2. Upsert the vectors in `vectors` in batches of **200 vectors per request asynchronously**, configuring **20 simultaneous requests**.\n",
        "3. Print the updated metrics of the `'datacamp-index'` Pinecone index.\n"
      ],
      "metadata": {
        "id": "DhLJ7xh-DD_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert vectors in batches of 200 vectors\n",
        "with pc.Index('datacamp-index', pool_threads = 20) as index:\n",
        "    async_results = [index.upsert(vectors=chunk, async_req = True) for chunk in chunks(vectors, batch_size=200)]\n",
        "    [async_result.get() for async_result in async_results]\n",
        "\n",
        "# Retrieve statistics of the connected Pinecone index\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6J-W8HHC2O5",
        "outputId": "3d40e02b-a6f9-43d7-c29f-5c04f284ed9e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'': {'vector_count': 1000},\n",
            "                'namespace1': {'vector_count': 50},\n",
            "                'namespace2': {'vector_count': 50}},\n",
            " 'total_vector_count': 1100,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upserting vectors for semantic search\n",
        "Time to embed some text data and upsert the vectors and metadata into your `'pinecone-datacamp'` index! You've been given a dataset named `squad_dataset.csv`, and a sample of **200 rows** has been loaded into the DataFrame, `df`.\n",
        "\n",
        "In this exercise, you will interact with the OpenAI API to use their embedding model. **You don't need to create or use your own API key**—a valid OpenAI client has been created for you and assigned to the `client` variable.\n",
        "\n",
        "Your task is to:\n",
        "- Embed the text using OpenAI's API.\n",
        "- Upsert the embeddings and metadata into the Pinecone index under the namespace **squad_dataset**.\n",
        "\n",
        "## Instructions\n",
        "1. **Initialize** the Pinecone client with your API key (**OpenAI client is already available as `client`**).\n",
        "2. **Extract** the `'id'`, `'text'`, and `'title'` metadata from each row in the batch.\n",
        "3. **Encode texts** using `'text-embedding-3-small'` from OpenAI with **dimensionality 1536**.\n",
        "4. **Upsert** the vectors and metadata into **Pinecone** under the namespace `'squad_dataset'`.\n"
      ],
      "metadata": {
        "id": "dEhbWsD3DxaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from uuid import uuid4\n",
        "import pinecone\n",
        "import time\n",
        "import openai\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "index = pc.Index('pinecone-datacamp')\n",
        "client = openai.OpenAI(api_key=\"OPENAI_key\")\n",
        "df = pd.read_csv(\"squad_dataset.csv\")\n",
        "\n",
        "# Define batch size\n",
        "batch_limit = 100\n",
        "\n",
        "# Function to process and upsert batches\n",
        "def process_and_upsert(batch):\n",
        "    \"\"\"Encodes texts, generates embeddings, and upserts them into Pinecone.\"\"\"\n",
        "    try:\n",
        "        # Extract metadata from each row\n",
        "        metadatas = [{\n",
        "            \"text_id\": str(row['id']),  # Ensure ID is a string\n",
        "            \"text\": row['text'],\n",
        "            \"title\": row['title']\n",
        "        } for _, row in batch.iterrows()]\n",
        "\n",
        "        # Convert texts to a list\n",
        "        texts = batch['text'].tolist()\n",
        "\n",
        "        # Generate unique IDs for each text\n",
        "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "\n",
        "        # Encode texts using OpenAI's embedding model\n",
        "        response = client.embeddings.create(input=texts, model=\"text-embedding-3-small\")\n",
        "        embeds = [np.array(x.embedding) for x in response.data]\n",
        "\n",
        "        # Ensure embeddings match text count\n",
        "        if len(embeds) != len(texts):\n",
        "            raise ValueError(\"Mismatch between embeddings and texts!\")\n",
        "\n",
        "        # Upsert vectors along with metadata into the specified namespace\n",
        "        index.upsert(vectors=list(zip(ids, embeds, metadatas)), namespace='squad_dataset')\n",
        "        print(f\"Successfully upserted {len(texts)} vectors.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {e}\")\n",
        "\n",
        "# Iterate over the DataFrame in batches\n",
        "for batch in np.array_split(df, max(1, len(df) // batch_limit)):\n",
        "    process_and_upsert(batch)\n",
        "    time.sleep(0.5)"
      ],
      "metadata": {
        "id": "fz_RCltcIg-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying vectors for semantic search  \n",
        "In this exercise, you'll create a query vector from the question, 'What is in front of the Notre Dame Main Building?'. Using this embedded query, you'll query the 'squad_dataset' namespace from the 'pinecone-datacamp' index and return the top five most similar vectors.  \n",
        "\n",
        "#### Instructions  \n",
        "\n",
        "- Initialize the Pinecone client with your API key (the OpenAI client is available as `client`).  \n",
        "- Create a query vector by embedding the query provided with the same OpenAI embedding model you used for embedding the other vectors.  \n",
        "- Query the \"squad_dataset\" namespace using `query_emb`, returning the top five most similar results.  \n"
      ],
      "metadata": {
        "id": "Dmd1grUKJEcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is in front of the Notre Dame Main Building?\"\n",
        "\n",
        "# Create the query vector\n",
        "query_response = client.embeddings.create(\n",
        "    input=query,\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "query_emb = query_response.data[0].embedding\n",
        "\n",
        "# Query the index and retrieve the top five most similar vectors\n",
        "retrieved_docs = index.query(\n",
        "    vector = query_emb,\n",
        "    top_k = 5,\n",
        "    namespace = 'squad_dataset',\n",
        ")\n",
        "\n",
        "for result in retrieved_docs['matches']:\n",
        "    print(f\"{result['id']}: {round(result['score'], 2)}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf1WzHQxIpM2",
        "outputId": "46789216-8236-498a-9d3d-c95c9a079111"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123b1f17-f34f-4b19-ba5f-0503f9353d5a: 0.46\n",
            "f64af2fb-616a-400e-b06d-2ff8f7f2a25d: 0.46\n",
            "2e85bf0f-eba9-40a7-aa82-25bff072bcdb: 0.46\n",
            "bd6280dc-9853-4849-acc2-e8f5ab7635ab: 0.46\n",
            "b29ed957-b6ac-4cdb-9279-4d42b748ebd2: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upserting YouTube transcripts  \n",
        "In this following exercise, you'll create a chatbot that can answer questions about YouTube videos by ingesting video transcripts and additional metadata into your 'pinecone-datacamp' index.  \n",
        "\n",
        "To start, you'll prepare data from the youtube_rag_data.csv file and upsert the vectors with all of their metadata into the 'pinecone-datacamp' index. The data is provided in the DataFrame `youtube_df`.  \n",
        "\n",
        "Here's an example transcript from the `youtube_df` DataFrame:  \n",
        "\n",
        "**id:**  \n",
        "35Pdoyi6ZoQ-t0.0  \n",
        "\n",
        "**title:**  \n",
        "Training and Testing an Italian BERT - Transformers From Scratch #4  \n",
        "\n",
        "**text:**  \n",
        "Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini-series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it...  \n",
        "\n",
        "**url:**  \n",
        "[https://youtu.be/35Pdoyi6ZoQ](https://youtu.be/35Pdoyi6ZoQ)  \n",
        "\n",
        "**published:**  \n",
        "01-01-2024  \n",
        "\n",
        "#### Instructions  \n",
        "- **Initialize** the Pinecone client with your API key (the OpenAI client is available as `client`).  \n",
        "- **Extract** the `'id'`, `'text'`, `'title'`, `'url'`, and `'published'` metadata from each row.  \n",
        "- **Encode** texts using `'text-embedding-3-small'` from OpenAI.  \n",
        "- **Upsert** the vectors and metadata to a namespace called `'youtube_rag_dataset'`.  \n"
      ],
      "metadata": {
        "id": "R7KY9d9aJlzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_limit = 100\n",
        "\n",
        "youtube_df = pd.read_csv(\"youtube_rag_data.csv\")\n",
        "\n",
        "for batch in np.array_split(youtube_df, len(youtube_df) / batch_limit):\n",
        "    # Extract the metadata from each row\n",
        "    metadatas = [{\n",
        "      \"text_id\": row['id'],\n",
        "      \"text\": row['text'],\n",
        "      \"title\": row['title'],\n",
        "      \"url\": row['url'],\n",
        "      \"published\": row['published']} for _, row in batch.iterrows()]\n",
        "    texts = batch['text'].tolist()\n",
        "\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "\n",
        "    # Encode texts using OpenAI\n",
        "    response = client.embeddings.create(input=texts, model=\"text-embedding-3-small\")\n",
        "    embeds = [np.array(x.embedding) for x in response.data]\n",
        "\n",
        "    # Upsert vectors to the correct namespace\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas), namespace='youtube_rag_dataset')\n",
        "\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oksB4fiOKHv5",
        "outputId": "87ddbdc9-2bea-4e9f-ff88-10330585a085"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension: 1536\n",
            "Index Fullness: 0.0\n",
            "Namespaces:\n",
            "  squad_dataset: 801 vectors\n",
            "  youtube_rag_dataset: 200 vectors\n",
            "Total Vector Count: 1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Retrieval Function\n",
        "\n",
        "## Overview\n",
        "A key process in the Retrieval Augmented Generation (RAG) workflow is retrieving data from the database. In this exercise, you'll design a custom function called `retrieve()` that will perform this crucial process in the final exercise of the course.\n",
        "\n",
        "## Instructions\n",
        "- **Initialize** the Pinecone client with your API key (the OpenAI client is available as `client`).\n",
        "- **Define** the function `retrieve()` that takes four parameters: `query`, `top_k`, `namespace`, and `emb_model`.\n",
        "- **Embed** the input query using the `emb_model` argument.\n",
        "- **Retrieve** the `top_k` similar vectors to `query_emb` with metadata, specifying the `namespace` provided to the function as an argument.\n"
      ],
      "metadata": {
        "id": "K6U-Ug1tKSN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a retrieve function that takes four arguments: query, top_k, namespace, and emb_model\n",
        "def retrieve(query, top_k, namespace, emb_model):\n",
        "    # Encode the input query using OpenAI\n",
        "    query_response = client.embeddings.create(\n",
        "        input=query,\n",
        "        model=emb_model\n",
        "    )\n",
        "\n",
        "    query_emb = query_response.data[0].embedding\n",
        "\n",
        "    # Query the index using the query_emb\n",
        "    docs = index.query(vector=query_emb, top_k=top_k, namespace=namespace,include_metadata=True)\n",
        "\n",
        "    retrieved_docs = []\n",
        "    sources = []\n",
        "    for doc in docs['matches']:\n",
        "        retrieved_docs.append(doc['metadata']['text'])\n",
        "        sources.append((doc['metadata']['title'], doc['metadata']['url']))\n",
        "\n",
        "    return retrieved_docs, sources\n",
        "\n",
        "documents, sources = retrieve(\n",
        "  query=\"How to build next-level Q&A with OpenAI\",\n",
        "  top_k=3,\n",
        "  namespace='youtube_rag_dataset',\n",
        "  emb_model=\"text-embedding-3-small\"\n",
        ")\n",
        "print(documents)\n",
        "print(sources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0mhglY4KaDh",
        "outputId": "2730fa7b-87eb-4ff6-e267-2798493e52d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer.\"\n",
            "\"And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering.\"\n",
            "[('How to Build Q&A Models in Python (Transformers)', 'https://youtu.be/scJsty_DR3o'), ('How to Build Q&A Models in Python (Transformers)', 'https://youtu.be/scJsty_DR3o'), ('How to Build Q&A Models in Python (Transformers)', 'https://youtu.be/scJsty_DR3o')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Question Answering Function\n",
        "\n",
        "## Overview\n",
        "You're almost there! The final piece in the Retrieval Augmented Generation (RAG) workflow is to integrate the retrieved documents with a question-answering model.\n",
        "\n",
        "A `prompt_with_context_builder()` function has already been defined and made available to you. This function takes the documents retrieved from the Pinecone index and integrates them into a prompt that the question-answering model can ingest:\n",
        "\n",
        "```python\n",
        "def prompt_with_context_builder(query, docs):\n",
        "    delim = '\\n\\n---\\n\\n'\n",
        "    prompt_start = 'Answer the question based on the context below.\\n\\nContext:\\n'\n",
        "    prompt_end = f'\\n\\nQuestion: {query}\\nAnswer:'\n",
        "\n",
        "    prompt = prompt_start + delim.join(docs) + prompt_end\n",
        "    return prompt\n",
        "```\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Initialize the Pinecone client with your API key (the OpenAI client is available as client).\n",
        "- Retrieve the three most similar documents to the query text from the 'youtube_rag_dataset' namespace.\n",
        "- Generate a response to the provided prompt and sys_prompt using OpenAI's 'gpt-4o-mini' model, specified using the chat_model function argument."
      ],
      "metadata": {
        "id": "pFXdfH3gLD5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to build next-level Q&A with OpenAI\"\n",
        "\n",
        "# Retrieve the top three most similar documents and their sources\n",
        "documents, sources = retrieve(query, top_k=3, namespace='youtube_rag_dataset', emb_model=\"text-embedding-3-small\")\n",
        "\n",
        "prompt_with_context = prompt_with_context_builder(query, documents)\n",
        "print(prompt_with_context)\n",
        "\n",
        "def question_answering(prompt, sources, chat_model):\n",
        "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
        "\n",
        "    # Use OpenAI chat completions to generate a response\n",
        "    res = client.chat.completions.create(\n",
        "        model=chat_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": sys_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    answer = res.choices[0].message.content.strip()\n",
        "    answer += \"\\n\\nSources:\"\n",
        "    for source in sources:\n",
        "        answer += \"\\n\" + source[0] + \": \" + source[1]\n",
        "\n",
        "    return answer\n",
        "\n",
        "answer = question_answering(\n",
        "  prompt=prompt_with_context,\n",
        "  sources=sources,\n",
        "  chat_model='gpt-4o-mini')\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLh47y-kLEJ0",
        "outputId": "2a5ab11f-6f54-4c4b-bca1-b3fdaa19bef8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below.\n",
            "\n",
            "Context:\n",
            "and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer.\n",
            "\n",
            "---\n",
            "\n",
            "and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer.\n",
            "\n",
            "---\n",
            "\n",
            "And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering.\n",
            "\n",
            "\n",
            "Question: How to build next-level Q&A with OpenAI\n",
            "Answer:\n",
            "The context provided does not specifically address building a next-level Q&A system with OpenAI. However, it does describe how to set up a question-answering pipeline using the Transformers library, which is a key component in building advanced Q&A systems. To build a next-level Q&A system with OpenAI, you would typically follow these steps:\n",
            "\n",
            "1. **Choose a Model**: Select a pre-trained model suitable for question answering from the Hugging Face Models page.\n",
            "\n",
            "2. **Initialize the Pipeline**: Use the Transformers library to create a Q&A pipeline by importing the pipeline function and specifying the model type as \"question answering\".\n",
            "\n",
            "3. **Prepare Input**: Structure your input to include a context and a question. The pipeline will handle the formatting required for question answering.\n",
            "\n",
            "4. **Fine-tuning (Optional)**: If you want to improve the model's performance for specific use cases, consider fine-tuning the model on a relevant dataset.\n",
            "\n",
            "5. **Integration**: Integrate the Q&A pipeline into your application, allowing users to input questions and receive answers based on the provided context.\n",
            "\n",
            "6. **Testing and Iteration**: Continuously test the system with various questions and contexts, iterating on the model and its parameters to enhance performance.\n",
            "\n",
            "By leveraging the capabilities of the Transformers library and OpenAI's models, you can create a robust and effective Q&A system.\n",
            "\n",
            "\n",
            "Sources:\n",
            "How to Build Q&A Models in Python (Transformers): https://youtu.be/scJsty_DR3o\n",
            "How to Build Q&A Models in Python (Transformers): https://youtu.be/scJsty_DR3o\n",
            "How to Build Q&A Models in Python (Transformers): https://youtu.be/scJsty_DR3o\n"
          ]
        }
      ]
    }
  ]
}
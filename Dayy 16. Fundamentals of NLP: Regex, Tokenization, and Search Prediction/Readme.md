# Day 16. 100 Days of Data Science Challenge - 02/16/2025

# Fundamentals of NLP: Regex, Tokenization, and Search Prediction

## ğŸ“ Project Overview  
Ever wondered how **news headlines** are structured or how **search engines predict your next word**?  
In this project, I dive deep into **Natural Language Processing (NLP)** techniques to analyze **New York Times headlines**, build an **autocompletion tool**, and compare text extraction with ChatGPT.  

ğŸ›  **Techniques Used:**  
âœ”ï¸ **Regex & Named Entity Recognition (NER)** for detecting proper nouns in headlines  
âœ”ï¸ **Text Preprocessing & Vocabulary Building** (Tokenization, Lemmatization, Frequency Analysis)  
âœ”ï¸ **Autocomplete Suggestion System** using **Levenshtein Distance**  
âœ”ï¸ **ChatGPT Comparison** â€“ Evaluating AI vs. custom NLP for text extraction  

---

## ğŸ“° Dataset: New York Times Headlines  
ğŸ“Œ **Source:** `nytimes_data_final.csv` (A collection of NYT headlines)  
ğŸ“Œ **Scope:** Extracting named entities, building vocabulary, and analyzing frequency patterns  

### **Sample Data**  
ğŸ“° "Biden Meets with World Leaders at UN Summit"  
ğŸ“° "Donald Trump Speaks at Rally Amid Controversy"  
ğŸ“° "Indiana University Researches AI in Healthcare"  

The dataset is processed to **identify named entities, extract unique words, and analyze word frequencies**.  

---

## ğŸ›  Project Breakdown  

### ğŸ”¹ **1. Regex & Named Entity Extraction**  
ğŸ“Œ **Goal:** Detect all **proper nouns (named entities)** in headlines  
ğŸ“Œ **Method:**  
   - Build **regular expressions** to extract names (e.g., "Joe Biden", "Donald Trump")  
   - Count **unique named entities**
     
ğŸ“Œ **Outcome:** Extracted **dozens of unique names** and printed headlines containing them  

### ğŸ”¹ **2. Vocabulary Building & Frequency Analysis**  
ğŸ“Œ **Goal:** Create a **cleaned vocabulary** from the headlines  
ğŸ“Œ **Steps:**  
   - Tokenize words  
   - Convert to **lowercase** (case folding)  
   - Apply **lemmatization** to get base forms  
   - Store unique words in a **vocabulary set**
     
ğŸ“Œ **Outcome:**  
   - Built a vocabulary of **unique words**  
   - Plotted a **word frequency distribution** for the **top 100 most common words**  

### ğŸ”¹ **3. Autocompletion Tool (Search Engine-Style Predictions)**  
ğŸ“Œ **Goal:** Predict **possible word completions** based on user input  
ğŸ“Œ **Method:**  
   - Load words from the **WordList.txt** file  
   - Use **Levenshtein Distance** to find the closest matches  
   - Return the **K most probable** words for autocompletion
      
ğŸ“Œ **Example:**  
   - Input: `"why i"`  
   - Output: `["why is", "why it", "why iâ€™m"]`  

### ğŸ”¹ **4. ChatGPT Comparison**  
ğŸ“Œ **Goal:** Compare **ChatGPT's performance** vs. my **Regex-based entity extraction**  
ğŸ“Œ **Steps:**  
   - Run **ChatGPT** on the same task (extract named entities from headlines)  
   - Analyze **differences in accuracy & recall**  
   - Document **strengths & weaknesses** of each approach
     
ğŸ“Œ **Findings:**  
   - **ChatGPT** is **better at contextual understanding** but sometimes **over-generates** names  
   - **Regex-based extraction** is **precise** but **misses variations & ambiguous cases**  
   - **Combining both** could create a **more powerful extraction pipeline**  

---

## ğŸ“Š Key Insights & Learnings  

âœ”ï¸ **Regex is powerful but limited** â€“ It struggles with **ambiguous names** or variations.  
âœ”ï¸ **Levenshtein Distance is useful for autocompletion**, but real-world search engines use **probabilistic models** (e.g., Markov Chains, Transformers).  
âœ”ï¸ **Frequency analysis reveals hidden patterns** â€“ Some words appear **significantly more** than others in news headlines.  
âœ”ï¸ **ChatGPT vs. Regex** â€“ AI models **understand context** but require validation for **accuracy**.  

---

## ğŸ“Œ Future Enhancements  

ğŸ”¹ **Improve Named Entity Recognition (NER)** using spaCy or NLTK.  
ğŸ”¹ **Use n-grams or Markov Models** to enhance autocompletion accuracy.  
ğŸ”¹ **Test ChatGPTâ€™s latest versions** to see improvements in text extraction.  
ğŸ”¹ **Expand to other text sources** (e.g., social media posts, scientific papers).  

---

## ğŸ’¡ Reflections  

This project was a **fascinating mix of NLP, search engine logic, and AI comparison**.  
From **building regex rules** to **evaluating AI performance**, I gained insights into how **language models process text** and how **autocompletion systems work behind the scenes**.  

ğŸ“° **Next Steps:**  
- Explore **transformer-based models** (e.g., BERT) for **contextual word predictions**.  
- Try **Named Entity Recognition (NER)** using **spaCy** for better accuracy.  
- Implement **word embeddings** to improve **autocomplete predictions**.  
